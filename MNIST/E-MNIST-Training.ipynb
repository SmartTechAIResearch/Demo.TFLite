{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (52800, 28, 28, 1)\n",
      "x_test: (8800, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    import gzip\n",
    "    labels_path = os.path.join(path,'%s-labels-idx1-ubyte.gz'% kind)\n",
    "    images_path = os.path.join(path,'%s-images-idx3-ubyte.gz'% kind)\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,offset=8)\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "#load and preprocess data\n",
    "\n",
    "dataset = 'emnist' # or 'emnist'\n",
    "output_layers = 10 # default, will be overridden\n",
    "\n",
    "\n",
    "if dataset == 'mnist':\n",
    "    output_layers = 10\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "elif dataset == 'emnist':\n",
    "    mapp = pd.read_csv(\"emnist/emnist-letters-mapping.txt\", delimiter = ' ', \\\n",
    "                   index_col=0, header=None, squeeze=True)\n",
    "\n",
    "    \n",
    "    emnist_distinct_upper = ['A', 'B', 'D', 'E', 'F', 'G', 'H', 'N', 'Q', 'R', 'T'] # Characters with distinct uppercase\n",
    "    emnist_nondistinct_upper = ['c', 'i', 'j', 'k', 'l', 'm', 'o', 'p', 's', 'u', 'v', 'w', 'x', 'y', 'z'] # Characters with non-distinct uppercase\n",
    "    emnist_nondistinct_upper_partial = ['c', 'i', 'k', 'l', 'm', 'o', 'p', 's', 'v', 'x', 'z'] # A selection of 11 characters to use as our model can not be too big\n",
    "\n",
    "                             # always uppercase  # Choose one of ('emnist_distinct_upper', 'emnist_nondistinct_upper_partial')\n",
    "    emnist_characters = [ord(i.upper()) for i in emnist_distinct_upper]\n",
    "\n",
    "                                        # COLUMN 1 is uppercase\n",
    "    emnist_characters_ascii = list(mapp[mapp[1].isin(emnist_characters)].index)\n",
    "\n",
    "    output_layers = len(emnist_characters_ascii)\n",
    "    x_train, y_train = load_mnist('emnist', kind='emnist-letters-train') # X stands for features, y for targets\n",
    "    x_test, y_test = load_mnist('emnist', kind='emnist-letters-test')\n",
    "    \n",
    "    train_mask = np.isin(y_train, emnist_characters_ascii)\n",
    "    x_train = x_train[train_mask]\n",
    "    y_train = y_train[train_mask] - 1\n",
    "    test_mask = np.isin(y_test, emnist_characters_ascii)\n",
    "    x_test = x_test[test_mask]\n",
    "    y_test = y_test[test_mask] - 1\n",
    "\n",
    "\n",
    "def rotate(image):\n",
    "    image = image.reshape(28, 28, 1)\n",
    "    image = np.fliplr(image)\n",
    "    image = np.rot90(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train = np.asarray(x_train)\n",
    "x_train = np.apply_along_axis(rotate, 1, x_train)\n",
    "print (\"x_train:\",x_train.shape)\n",
    "\n",
    "x_test = np.asarray(x_test)\n",
    "x_test = np.apply_along_axis(rotate, 1, x_test)\n",
    "print (\"x_test:\",x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_labels, y_train_values = np.unique(y_train, return_inverse=True)\n",
    "y_test_labels, y_test_values = np.unique(y_test, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'D', 'E', 'F', 'G', 'H', 'N', 'Q', 'R', 'T']\n"
     ]
    }
   ],
   "source": [
    "LABELS = [chr(mapp.iloc[i][1]) for i in y_train_labels]\n",
    "\n",
    "print(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  3,  4,  5,  6,  7, 13, 16, 17, 19], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-eff7321a57f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_train_values\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m## Reconstruct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_labels' is not defined"
     ]
    }
   ],
   "source": [
    "y_train_labels[y_train_values] ## Reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1352)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11)                14883     \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (None, 11)                0         \n",
      "=================================================================\n",
      "Total params: 14,963\n",
      "Trainable params: 14,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# x_train = x_train.reshape(x_train.shape[0],28,28,1)\n",
    "# x_test = x_test.reshape(x_test.shape[0],28,28,1)\n",
    "\n",
    "#create model\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(8, kernel_size=(3,3), strides=(1,1),\n",
    "                         activation='relu',input_shape=(28,28,1)),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(output_layers),\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "100/100 - 1s - loss: 2.1680 - accuracy: 0.4627 - val_loss: 1.9492 - val_accuracy: 0.6414\n",
      "Epoch 2/150\n",
      "100/100 - 1s - loss: 1.9007 - accuracy: 0.6727 - val_loss: 1.8799 - val_accuracy: 0.6845\n",
      "Epoch 3/150\n",
      "100/100 - 1s - loss: 1.8751 - accuracy: 0.6867 - val_loss: 1.8615 - val_accuracy: 0.6963\n",
      "Epoch 4/150\n",
      "100/100 - 1s - loss: 1.8445 - accuracy: 0.7155 - val_loss: 1.8444 - val_accuracy: 0.7098\n",
      "Epoch 5/150\n",
      "100/100 - 1s - loss: 1.8399 - accuracy: 0.7155 - val_loss: 1.8316 - val_accuracy: 0.7210\n",
      "Epoch 6/150\n",
      "100/100 - 1s - loss: 1.8231 - accuracy: 0.7334 - val_loss: 1.8247 - val_accuracy: 0.7287\n",
      "Epoch 7/150\n",
      "100/100 - 1s - loss: 1.8202 - accuracy: 0.7302 - val_loss: 1.8179 - val_accuracy: 0.7352\n",
      "Epoch 8/150\n",
      "100/100 - 1s - loss: 1.8156 - accuracy: 0.7361 - val_loss: 1.8061 - val_accuracy: 0.7464\n",
      "Epoch 9/150\n",
      "100/100 - 1s - loss: 1.8073 - accuracy: 0.7442 - val_loss: 1.8040 - val_accuracy: 0.7491\n",
      "Epoch 10/150\n",
      "100/100 - 1s - loss: 1.8033 - accuracy: 0.7477 - val_loss: 1.7989 - val_accuracy: 0.7538\n",
      "Epoch 11/150\n",
      "100/100 - 1s - loss: 1.7924 - accuracy: 0.7617 - val_loss: 1.7906 - val_accuracy: 0.7635\n",
      "Epoch 12/150\n",
      "100/100 - 1s - loss: 1.7849 - accuracy: 0.7653 - val_loss: 1.7836 - val_accuracy: 0.7689\n",
      "Epoch 13/150\n",
      "100/100 - 1s - loss: 1.7754 - accuracy: 0.7786 - val_loss: 1.7799 - val_accuracy: 0.7707\n",
      "Epoch 14/150\n",
      "100/100 - 1s - loss: 1.7755 - accuracy: 0.7791 - val_loss: 1.7702 - val_accuracy: 0.7826\n",
      "Epoch 15/150\n",
      "100/100 - 1s - loss: 1.7716 - accuracy: 0.7839 - val_loss: 1.7643 - val_accuracy: 0.7900\n",
      "Epoch 16/150\n",
      "100/100 - 1s - loss: 1.7453 - accuracy: 0.8095 - val_loss: 1.7569 - val_accuracy: 0.7982\n",
      "Epoch 17/150\n",
      "100/100 - 1s - loss: 1.7512 - accuracy: 0.8070 - val_loss: 1.7531 - val_accuracy: 0.8040\n",
      "Epoch 18/150\n",
      "100/100 - 1s - loss: 1.7452 - accuracy: 0.8109 - val_loss: 1.7485 - val_accuracy: 0.8075\n",
      "Epoch 19/150\n",
      "100/100 - 1s - loss: 1.7403 - accuracy: 0.8161 - val_loss: 1.7443 - val_accuracy: 0.8125\n",
      "Epoch 20/150\n",
      "100/100 - 1s - loss: 1.7397 - accuracy: 0.8152 - val_loss: 1.7439 - val_accuracy: 0.8126\n",
      "Epoch 21/150\n",
      "100/100 - 1s - loss: 1.7412 - accuracy: 0.8155 - val_loss: 1.7380 - val_accuracy: 0.8200\n",
      "Epoch 22/150\n",
      "100/100 - 1s - loss: 1.7283 - accuracy: 0.8303 - val_loss: 1.7343 - val_accuracy: 0.8206\n",
      "Epoch 23/150\n",
      "100/100 - 1s - loss: 1.7312 - accuracy: 0.8242 - val_loss: 1.7328 - val_accuracy: 0.8219\n",
      "Epoch 24/150\n",
      "100/100 - 1s - loss: 1.7285 - accuracy: 0.8297 - val_loss: 1.7310 - val_accuracy: 0.8260\n",
      "Epoch 25/150\n",
      "100/100 - 1s - loss: 1.7185 - accuracy: 0.8383 - val_loss: 1.7245 - val_accuracy: 0.8333\n",
      "Epoch 26/150\n",
      "100/100 - 1s - loss: 1.7244 - accuracy: 0.8294 - val_loss: 1.7238 - val_accuracy: 0.8323\n",
      "Epoch 27/150\n",
      "100/100 - 1s - loss: 1.7165 - accuracy: 0.8409 - val_loss: 1.7202 - val_accuracy: 0.8364\n",
      "Epoch 28/150\n",
      "100/100 - 1s - loss: 1.7153 - accuracy: 0.8419 - val_loss: 1.7185 - val_accuracy: 0.8393\n",
      "Epoch 29/150\n",
      "100/100 - 1s - loss: 1.7129 - accuracy: 0.8475 - val_loss: 1.7174 - val_accuracy: 0.8402\n",
      "Epoch 30/150\n",
      "100/100 - 1s - loss: 1.7166 - accuracy: 0.8431 - val_loss: 1.7147 - val_accuracy: 0.8410\n",
      "Epoch 31/150\n",
      "100/100 - 1s - loss: 1.7080 - accuracy: 0.8492 - val_loss: 1.7162 - val_accuracy: 0.8401\n",
      "Epoch 32/150\n",
      "100/100 - 1s - loss: 1.7026 - accuracy: 0.8534 - val_loss: 1.7134 - val_accuracy: 0.8427\n",
      "Epoch 33/150\n",
      "100/100 - 1s - loss: 1.7031 - accuracy: 0.8545 - val_loss: 1.7102 - val_accuracy: 0.8450\n",
      "Epoch 34/150\n",
      "100/100 - 1s - loss: 1.7079 - accuracy: 0.8480 - val_loss: 1.7105 - val_accuracy: 0.8439\n",
      "Epoch 35/150\n",
      "100/100 - 1s - loss: 1.6984 - accuracy: 0.8594 - val_loss: 1.7121 - val_accuracy: 0.8433\n",
      "Epoch 36/150\n",
      "100/100 - 1s - loss: 1.6967 - accuracy: 0.8592 - val_loss: 1.7047 - val_accuracy: 0.8492\n",
      "Epoch 37/150\n",
      "100/100 - 1s - loss: 1.6987 - accuracy: 0.8544 - val_loss: 1.7065 - val_accuracy: 0.8467\n",
      "Epoch 38/150\n",
      "100/100 - 1s - loss: 1.6982 - accuracy: 0.8566 - val_loss: 1.7117 - val_accuracy: 0.8401\n",
      "Epoch 39/150\n",
      "100/100 - 1s - loss: 1.6983 - accuracy: 0.8581 - val_loss: 1.7053 - val_accuracy: 0.8497\n",
      "Epoch 40/150\n",
      "100/100 - 1s - loss: 1.6953 - accuracy: 0.8598 - val_loss: 1.7027 - val_accuracy: 0.8518\n",
      "Epoch 41/150\n",
      "100/100 - 1s - loss: 1.6932 - accuracy: 0.8617 - val_loss: 1.7058 - val_accuracy: 0.8474\n",
      "Epoch 42/150\n",
      "100/100 - 1s - loss: 1.6932 - accuracy: 0.8611 - val_loss: 1.7013 - val_accuracy: 0.8535\n",
      "Epoch 43/150\n",
      "100/100 - 1s - loss: 1.6875 - accuracy: 0.8664 - val_loss: 1.7016 - val_accuracy: 0.8503\n",
      "Epoch 44/150\n",
      "100/100 - 1s - loss: 1.6966 - accuracy: 0.8572 - val_loss: 1.7009 - val_accuracy: 0.8509\n",
      "Epoch 45/150\n",
      "100/100 - 1s - loss: 1.6893 - accuracy: 0.8655 - val_loss: 1.6978 - val_accuracy: 0.8573\n",
      "Epoch 46/150\n",
      "100/100 - 1s - loss: 1.6860 - accuracy: 0.8687 - val_loss: 1.6981 - val_accuracy: 0.8552\n",
      "Epoch 47/150\n",
      "100/100 - 1s - loss: 1.6889 - accuracy: 0.8652 - val_loss: 1.6973 - val_accuracy: 0.8569\n",
      "Epoch 48/150\n",
      "100/100 - 1s - loss: 1.6863 - accuracy: 0.8667 - val_loss: 1.6968 - val_accuracy: 0.8568\n",
      "Epoch 49/150\n",
      "100/100 - 1s - loss: 1.6843 - accuracy: 0.8733 - val_loss: 1.6957 - val_accuracy: 0.8577\n",
      "Epoch 50/150\n",
      "100/100 - 1s - loss: 1.6856 - accuracy: 0.8670 - val_loss: 1.6954 - val_accuracy: 0.8560\n",
      "Epoch 51/150\n",
      "100/100 - 1s - loss: 1.6864 - accuracy: 0.8652 - val_loss: 1.6956 - val_accuracy: 0.8569\n",
      "Epoch 52/150\n",
      "100/100 - 1s - loss: 1.6871 - accuracy: 0.8655 - val_loss: 1.6936 - val_accuracy: 0.8598\n",
      "Epoch 53/150\n",
      "100/100 - 1s - loss: 1.6836 - accuracy: 0.8712 - val_loss: 1.6950 - val_accuracy: 0.8567\n",
      "Epoch 54/150\n",
      "100/100 - 1s - loss: 1.6809 - accuracy: 0.8731 - val_loss: 1.6918 - val_accuracy: 0.8630\n",
      "Epoch 55/150\n",
      "100/100 - 1s - loss: 1.6842 - accuracy: 0.8692 - val_loss: 1.6904 - val_accuracy: 0.8619\n",
      "Epoch 56/150\n",
      "100/100 - 1s - loss: 1.6849 - accuracy: 0.8662 - val_loss: 1.6942 - val_accuracy: 0.8569\n",
      "Epoch 57/150\n",
      "100/100 - 1s - loss: 1.6872 - accuracy: 0.8662 - val_loss: 1.6898 - val_accuracy: 0.8632\n",
      "Epoch 58/150\n",
      "100/100 - 1s - loss: 1.6855 - accuracy: 0.8667 - val_loss: 1.6895 - val_accuracy: 0.8639\n",
      "Epoch 59/150\n",
      "100/100 - 1s - loss: 1.6856 - accuracy: 0.8684 - val_loss: 1.6918 - val_accuracy: 0.8619\n",
      "Epoch 60/150\n",
      "100/100 - 1s - loss: 1.6782 - accuracy: 0.8745 - val_loss: 1.6919 - val_accuracy: 0.8591\n",
      "Epoch 61/150\n",
      "100/100 - 1s - loss: 1.6812 - accuracy: 0.8703 - val_loss: 1.6924 - val_accuracy: 0.8581\n",
      "Epoch 62/150\n",
      "100/100 - 1s - loss: 1.6886 - accuracy: 0.8625 - val_loss: 1.6897 - val_accuracy: 0.8607\n",
      "Epoch 63/150\n",
      "100/100 - 1s - loss: 1.6841 - accuracy: 0.8650 - val_loss: 1.6887 - val_accuracy: 0.8641\n",
      "Epoch 64/150\n",
      "100/100 - 1s - loss: 1.6793 - accuracy: 0.8745 - val_loss: 1.6893 - val_accuracy: 0.8617\n",
      "Epoch 65/150\n",
      "100/100 - 1s - loss: 1.6835 - accuracy: 0.8687 - val_loss: 1.6909 - val_accuracy: 0.8614\n",
      "Epoch 66/150\n",
      "100/100 - 1s - loss: 1.6776 - accuracy: 0.8750 - val_loss: 1.6872 - val_accuracy: 0.8657\n",
      "Epoch 67/150\n",
      "100/100 - 1s - loss: 1.6836 - accuracy: 0.8673 - val_loss: 1.6880 - val_accuracy: 0.8619\n",
      "Epoch 68/150\n",
      "100/100 - 1s - loss: 1.6746 - accuracy: 0.8764 - val_loss: 1.6911 - val_accuracy: 0.8585\n",
      "Epoch 69/150\n",
      "100/100 - 1s - loss: 1.6785 - accuracy: 0.8736 - val_loss: 1.6881 - val_accuracy: 0.8631\n",
      "Epoch 70/150\n",
      "100/100 - 1s - loss: 1.6802 - accuracy: 0.8703 - val_loss: 1.6905 - val_accuracy: 0.8594\n",
      "Epoch 71/150\n",
      "100/100 - 1s - loss: 1.6800 - accuracy: 0.8698 - val_loss: 1.6904 - val_accuracy: 0.8608\n",
      "Epoch 72/150\n",
      "100/100 - 1s - loss: 1.6797 - accuracy: 0.8728 - val_loss: 1.6914 - val_accuracy: 0.8580\n",
      "Epoch 73/150\n",
      "100/100 - 1s - loss: 1.6771 - accuracy: 0.8748 - val_loss: 1.6872 - val_accuracy: 0.8628\n",
      "Epoch 74/150\n",
      "100/100 - 1s - loss: 1.6806 - accuracy: 0.8691 - val_loss: 1.6881 - val_accuracy: 0.8634\n",
      "Epoch 75/150\n",
      "100/100 - 1s - loss: 1.6767 - accuracy: 0.8739 - val_loss: 1.6849 - val_accuracy: 0.8660\n",
      "Epoch 76/150\n",
      "100/100 - 1s - loss: 1.6780 - accuracy: 0.8709 - val_loss: 1.6865 - val_accuracy: 0.8647\n",
      "Epoch 77/150\n",
      "100/100 - 1s - loss: 1.6746 - accuracy: 0.8750 - val_loss: 1.6842 - val_accuracy: 0.8639\n",
      "Epoch 78/150\n",
      "100/100 - 1s - loss: 1.6775 - accuracy: 0.8737 - val_loss: 1.6899 - val_accuracy: 0.8592\n",
      "Epoch 79/150\n",
      "100/100 - 1s - loss: 1.6712 - accuracy: 0.8820 - val_loss: 1.6859 - val_accuracy: 0.8641\n",
      "Epoch 80/150\n",
      "100/100 - 1s - loss: 1.6728 - accuracy: 0.8778 - val_loss: 1.6826 - val_accuracy: 0.8660\n",
      "Epoch 81/150\n",
      "100/100 - 1s - loss: 1.6743 - accuracy: 0.8767 - val_loss: 1.6818 - val_accuracy: 0.8686\n",
      "Epoch 82/150\n",
      "100/100 - 1s - loss: 1.6758 - accuracy: 0.8752 - val_loss: 1.6834 - val_accuracy: 0.8658\n",
      "Epoch 83/150\n",
      "100/100 - 1s - loss: 1.6684 - accuracy: 0.8823 - val_loss: 1.6849 - val_accuracy: 0.8652\n",
      "Epoch 84/150\n",
      "100/100 - 1s - loss: 1.6740 - accuracy: 0.8750 - val_loss: 1.6841 - val_accuracy: 0.8658\n",
      "Epoch 85/150\n",
      "100/100 - 1s - loss: 1.6755 - accuracy: 0.8769 - val_loss: 1.6840 - val_accuracy: 0.8643\n",
      "Epoch 86/150\n",
      "100/100 - 1s - loss: 1.6687 - accuracy: 0.8827 - val_loss: 1.6848 - val_accuracy: 0.8642\n",
      "Epoch 87/150\n",
      "100/100 - 1s - loss: 1.6676 - accuracy: 0.8838 - val_loss: 1.6819 - val_accuracy: 0.8673\n",
      "Epoch 88/150\n",
      "100/100 - 1s - loss: 1.6676 - accuracy: 0.8827 - val_loss: 1.6805 - val_accuracy: 0.8698\n",
      "Epoch 89/150\n",
      "100/100 - 1s - loss: 1.6723 - accuracy: 0.8778 - val_loss: 1.6810 - val_accuracy: 0.8685\n",
      "Epoch 90/150\n",
      "100/100 - 1s - loss: 1.6646 - accuracy: 0.8850 - val_loss: 1.6825 - val_accuracy: 0.8665\n",
      "Epoch 91/150\n",
      "100/100 - 1s - loss: 1.6682 - accuracy: 0.8811 - val_loss: 1.6809 - val_accuracy: 0.8689\n",
      "Epoch 92/150\n",
      "100/100 - 1s - loss: 1.6642 - accuracy: 0.8858 - val_loss: 1.6827 - val_accuracy: 0.8665\n",
      "Epoch 93/150\n",
      "100/100 - 1s - loss: 1.6656 - accuracy: 0.8853 - val_loss: 1.6827 - val_accuracy: 0.8680\n",
      "Epoch 94/150\n",
      "100/100 - 1s - loss: 1.6656 - accuracy: 0.8866 - val_loss: 1.6815 - val_accuracy: 0.8670\n",
      "Epoch 95/150\n",
      "100/100 - 1s - loss: 1.6648 - accuracy: 0.8867 - val_loss: 1.6806 - val_accuracy: 0.8710\n",
      "Epoch 96/150\n",
      "100/100 - 1s - loss: 1.6710 - accuracy: 0.8802 - val_loss: 1.6811 - val_accuracy: 0.8691\n",
      "Epoch 97/150\n",
      "100/100 - 1s - loss: 1.6604 - accuracy: 0.8898 - val_loss: 1.6795 - val_accuracy: 0.8711\n",
      "Epoch 98/150\n",
      "100/100 - 1s - loss: 1.6677 - accuracy: 0.8833 - val_loss: 1.6787 - val_accuracy: 0.8708\n",
      "Epoch 99/150\n",
      "100/100 - 1s - loss: 1.6695 - accuracy: 0.8823 - val_loss: 1.6774 - val_accuracy: 0.8720\n",
      "Epoch 100/150\n",
      "100/100 - 1s - loss: 1.6653 - accuracy: 0.8855 - val_loss: 1.6783 - val_accuracy: 0.8707\n",
      "Epoch 101/150\n",
      "100/100 - 1s - loss: 1.6671 - accuracy: 0.8842 - val_loss: 1.6784 - val_accuracy: 0.8724\n",
      "Epoch 102/150\n",
      "100/100 - 1s - loss: 1.6654 - accuracy: 0.8852 - val_loss: 1.6787 - val_accuracy: 0.8686\n",
      "Epoch 103/150\n",
      "100/100 - 1s - loss: 1.6639 - accuracy: 0.8859 - val_loss: 1.6756 - val_accuracy: 0.8732\n",
      "Epoch 104/150\n",
      "100/100 - 1s - loss: 1.6617 - accuracy: 0.8909 - val_loss: 1.6783 - val_accuracy: 0.8701\n",
      "Epoch 105/150\n",
      "100/100 - 1s - loss: 1.6569 - accuracy: 0.8922 - val_loss: 1.6770 - val_accuracy: 0.8717\n",
      "Epoch 106/150\n",
      "100/100 - 1s - loss: 1.6563 - accuracy: 0.8955 - val_loss: 1.6723 - val_accuracy: 0.8776\n",
      "Epoch 107/150\n",
      "100/100 - 1s - loss: 1.6574 - accuracy: 0.8939 - val_loss: 1.6722 - val_accuracy: 0.8758\n",
      "Epoch 108/150\n",
      "100/100 - 1s - loss: 1.6577 - accuracy: 0.8936 - val_loss: 1.6723 - val_accuracy: 0.8764\n",
      "Epoch 109/150\n",
      "100/100 - 1s - loss: 1.6520 - accuracy: 0.8970 - val_loss: 1.6695 - val_accuracy: 0.8791\n",
      "Epoch 110/150\n",
      "100/100 - 1s - loss: 1.6543 - accuracy: 0.8963 - val_loss: 1.6696 - val_accuracy: 0.8808\n",
      "Epoch 111/150\n",
      "100/100 - 1s - loss: 1.6558 - accuracy: 0.8952 - val_loss: 1.6735 - val_accuracy: 0.8737\n",
      "Epoch 112/150\n",
      "100/100 - 1s - loss: 1.6527 - accuracy: 0.8992 - val_loss: 1.6674 - val_accuracy: 0.8814\n",
      "Epoch 113/150\n",
      "100/100 - 1s - loss: 1.6476 - accuracy: 0.9023 - val_loss: 1.6674 - val_accuracy: 0.8807\n",
      "Epoch 114/150\n",
      "100/100 - 1s - loss: 1.6540 - accuracy: 0.8953 - val_loss: 1.6669 - val_accuracy: 0.8792\n",
      "Epoch 115/150\n",
      "100/100 - 1s - loss: 1.6466 - accuracy: 0.9048 - val_loss: 1.6662 - val_accuracy: 0.8816\n",
      "Epoch 116/150\n",
      "100/100 - 1s - loss: 1.6470 - accuracy: 0.9027 - val_loss: 1.6623 - val_accuracy: 0.8875\n",
      "Epoch 117/150\n",
      "100/100 - 1s - loss: 1.6503 - accuracy: 0.9008 - val_loss: 1.6650 - val_accuracy: 0.8828\n",
      "Epoch 118/150\n",
      "100/100 - 1s - loss: 1.6503 - accuracy: 0.8994 - val_loss: 1.6627 - val_accuracy: 0.8849\n",
      "Epoch 119/150\n",
      "100/100 - 1s - loss: 1.6486 - accuracy: 0.9000 - val_loss: 1.6664 - val_accuracy: 0.8810\n",
      "Epoch 120/150\n",
      "100/100 - 1s - loss: 1.6425 - accuracy: 0.9084 - val_loss: 1.6627 - val_accuracy: 0.8841\n",
      "Epoch 121/150\n",
      "100/100 - 1s - loss: 1.6489 - accuracy: 0.9022 - val_loss: 1.6665 - val_accuracy: 0.8824\n",
      "Epoch 122/150\n",
      "100/100 - 1s - loss: 1.6461 - accuracy: 0.9056 - val_loss: 1.6614 - val_accuracy: 0.8869\n",
      "Epoch 123/150\n",
      "100/100 - 1s - loss: 1.6486 - accuracy: 0.9014 - val_loss: 1.6639 - val_accuracy: 0.8843\n",
      "Epoch 124/150\n",
      "100/100 - 1s - loss: 1.6452 - accuracy: 0.9045 - val_loss: 1.6640 - val_accuracy: 0.8848\n",
      "Epoch 125/150\n",
      "100/100 - 1s - loss: 1.6494 - accuracy: 0.8998 - val_loss: 1.6616 - val_accuracy: 0.8874\n",
      "Epoch 126/150\n",
      "100/100 - 1s - loss: 1.6466 - accuracy: 0.9020 - val_loss: 1.6606 - val_accuracy: 0.8877\n",
      "Epoch 127/150\n",
      "100/100 - 1s - loss: 1.6431 - accuracy: 0.9083 - val_loss: 1.6620 - val_accuracy: 0.8859\n",
      "Epoch 128/150\n",
      "100/100 - 1s - loss: 1.6449 - accuracy: 0.9064 - val_loss: 1.6630 - val_accuracy: 0.8844\n",
      "Epoch 129/150\n",
      "100/100 - 1s - loss: 1.6460 - accuracy: 0.9038 - val_loss: 1.6609 - val_accuracy: 0.8865\n",
      "Epoch 130/150\n",
      "100/100 - 1s - loss: 1.6445 - accuracy: 0.9042 - val_loss: 1.6630 - val_accuracy: 0.8856\n",
      "Epoch 131/150\n",
      "100/100 - 1s - loss: 1.6427 - accuracy: 0.9094 - val_loss: 1.6599 - val_accuracy: 0.8873\n",
      "Epoch 132/150\n",
      "100/100 - 1s - loss: 1.6467 - accuracy: 0.9019 - val_loss: 1.6617 - val_accuracy: 0.8856\n",
      "Epoch 133/150\n",
      "100/100 - 1s - loss: 1.6418 - accuracy: 0.9078 - val_loss: 1.6615 - val_accuracy: 0.8853\n",
      "Epoch 134/150\n",
      "100/100 - 1s - loss: 1.6461 - accuracy: 0.9030 - val_loss: 1.6593 - val_accuracy: 0.8894\n",
      "Epoch 135/150\n",
      "100/100 - 1s - loss: 1.6390 - accuracy: 0.9131 - val_loss: 1.6577 - val_accuracy: 0.8894\n",
      "Epoch 136/150\n",
      "100/100 - 1s - loss: 1.6446 - accuracy: 0.9053 - val_loss: 1.6617 - val_accuracy: 0.8842\n",
      "Epoch 137/150\n",
      "100/100 - 1s - loss: 1.6421 - accuracy: 0.9084 - val_loss: 1.6587 - val_accuracy: 0.8878\n",
      "Epoch 138/150\n",
      "100/100 - 1s - loss: 1.6435 - accuracy: 0.9073 - val_loss: 1.6597 - val_accuracy: 0.8882\n",
      "Epoch 139/150\n",
      "100/100 - 1s - loss: 1.6436 - accuracy: 0.9069 - val_loss: 1.6602 - val_accuracy: 0.8868\n",
      "Epoch 140/150\n",
      "100/100 - 1s - loss: 1.6367 - accuracy: 0.9141 - val_loss: 1.6583 - val_accuracy: 0.8877\n",
      "Epoch 141/150\n",
      "100/100 - 1s - loss: 1.6341 - accuracy: 0.9150 - val_loss: 1.6614 - val_accuracy: 0.8840\n",
      "Epoch 142/150\n",
      "100/100 - 1s - loss: 1.6330 - accuracy: 0.9161 - val_loss: 1.6599 - val_accuracy: 0.8881\n",
      "Epoch 143/150\n",
      "100/100 - 1s - loss: 1.6415 - accuracy: 0.9066 - val_loss: 1.6599 - val_accuracy: 0.8877\n",
      "Epoch 144/150\n",
      "100/100 - 1s - loss: 1.6380 - accuracy: 0.9117 - val_loss: 1.6571 - val_accuracy: 0.8917\n",
      "Epoch 145/150\n",
      "100/100 - 1s - loss: 1.6404 - accuracy: 0.9108 - val_loss: 1.6623 - val_accuracy: 0.8843\n",
      "Epoch 146/150\n",
      "100/100 - 1s - loss: 1.6418 - accuracy: 0.9078 - val_loss: 1.6597 - val_accuracy: 0.8875\n",
      "Epoch 147/150\n",
      "100/100 - 1s - loss: 1.6383 - accuracy: 0.9125 - val_loss: 1.6574 - val_accuracy: 0.8902\n",
      "Epoch 148/150\n",
      "100/100 - 1s - loss: 1.6337 - accuracy: 0.9152 - val_loss: 1.6579 - val_accuracy: 0.8877\n",
      "Epoch 149/150\n",
      "100/100 - 1s - loss: 1.6343 - accuracy: 0.9150 - val_loss: 1.6550 - val_accuracy: 0.8914\n",
      "Epoch 150/150\n",
      "100/100 - 1s - loss: 1.6372 - accuracy: 0.9105 - val_loss: 1.6566 - val_accuracy: 0.8890\n"
     ]
    }
   ],
   "source": [
    "#add datagen to make the model more accepting for the 'mouse' written digits\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False, # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.0, # Randomly zoom image \n",
    "        width_shift_range=0.0,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.0,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "datagen.fit(x_train)\n",
    "\n",
    "#Train the model using the augmentation datagenerator\n",
    "history = model.fit(\n",
    "      datagen.flow(x_train,y_train_values, batch_size=64),\n",
    "      validation_data=(x_test, y_test_values),\n",
    "      steps_per_epoch=100,\n",
    "      epochs=150,\n",
    "      verbose=2)\n",
    "\n",
    "\n",
    "#Save the resulted model\n",
    "model.save(dataset + \"-model_augm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_1 (QuantizeLa (None, 28, 28, 1)         3         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_1 (QuantizeWrap (None, 26, 26, 8)         99        \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_1 (Quant (None, 13, 13, 8)         1         \n",
      "_________________________________________________________________\n",
      "quant_flatten_1 (QuantizeWra (None, 1352)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense_1 (QuantizeWrapp (None, 11)                14888     \n",
      "_________________________________________________________________\n",
      "quant_softmax_1 (QuantizeWra (None, 11)                1         \n",
      "=================================================================\n",
      "Total params: 14,993\n",
      "Trainable params: 14,963\n",
      "Non-trainable params: 30\n",
      "_________________________________________________________________\n",
      "96/96 [==============================] - 4s 35ms/step - loss: 1.6572 - accuracy: 0.9029 - val_loss: 1.6370 - val_accuracy: 0.9144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as conv2d_1_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, flatten_1_layer_call_and_return_conditional_losses, flatten_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as conv2d_1_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, flatten_1_layer_call_and_return_conditional_losses, flatten_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\natha\\AppData\\Local\\Temp\\tmp8wb7itat\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\natha\\AppData\\Local\\Temp\\tmp8wb7itat\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18376"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------------------------------------------------------\n",
    "#or quantize aware training to directly convert it to a tflite model\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# 'quantize_model' requires a recompile.\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()\n",
    "\n",
    "train_images_subset = x_train\n",
    "train_labels_subset = y_train_values\n",
    "\n",
    "q_aware_model.fit(train_images_subset, train_labels_subset,\n",
    "                  batch_size=500, epochs=1, validation_split=0.1)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "quantized_tflite_model = converter.convert()\n",
    "open(dataset + \"-quantized_model.tflite\", \"wb\").write(quantized_tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tflite]",
   "language": "python",
   "name": "conda-env-.conda-tflite-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
